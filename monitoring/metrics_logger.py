# monitoring/metrics_logger_fixed.py
import pandas as pd
import numpy as np
import joblib
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
from sqlalchemy import create_engine, text
from datetime import datetime
from evidently.report import Report
from evidently.metrics import DatasetDriftMetric
import json

# ‚úÖ Load your trained model
def load_model(path="models/best_model.pkl"):
    return joblib.load(path)

# ‚úÖ Load actual training and test data
def load_actual_data():
    """Load your actual training and test data"""
    print("üìÅ Loading actual training and test data...")
    
    try:
        # Load training data
        X_train = pd.read_csv("data/features/X_train.csv")
        y_train = pd.read_csv("data/features/y_train.csv")
        
        # Load test data  
        X_test = pd.read_csv("data/features/X_test.csv")
        y_test = pd.read_csv("data/features/y_test.csv")
        
        # Handle column names
        if y_train.shape[1] == 1:
            y_train = y_train.iloc[:, 0]
        if y_test.shape[1] == 1:
            y_test = y_test.iloc[:, 0]
            
        print(f"‚úÖ Training data: {X_train.shape}, Test data: {X_test.shape}")
        print(f"üéØ Training target distribution: {y_train.value_counts().to_dict()}")
        print(f"üéØ Test target distribution: {y_test.value_counts().to_dict()}")
        
        return X_train, y_train, X_test, y_test
        
    except Exception as e:
        print(f"‚ùå ERROR loading data: {e}")
        return None, None, None, None

# ‚úÖ Calculate performance metrics
def calculate_model_metrics(model, X_test, y_test):
    print("üîç Making predictions...")
    preds = model.predict(X_test)
    
    print(f"üìä Predictions shape: {preds.shape}")
    print(f"üìä Predictions unique: {np.unique(preds, return_counts=True)}")
    print(f"üìä Actual target unique: {y_test.value_counts().to_dict()}")
    
    if preds.ndim > 1:
        preds = preds.argmax(axis=1)
    
    metrics = {
        "precision": precision_score(y_test, preds, zero_division=0),
        "recall": recall_score(y_test, preds, zero_division=0),
        "f1": f1_score(y_test, preds, zero_division=0),
        "auc": roc_auc_score(y_test, preds)
    }
    
    print("üìà Model Performance Metrics:")
    for key, value in metrics.items():
        print(f"  {key}: {value:.4f}")
    
    return metrics

# ‚úÖ Log to model_performance table (FIXED with backticks)
def log_to_model_performance(metrics, engine):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with engine.begin() as conn:
            conn.execute(
                text("""
                    INSERT INTO model_performance 
                    (timestamp, model_name, `precision`, `recall`, `f1`, `auc`, `details`)
                    VALUES (:timestamp, :model_name, :precision, :recall, :f1, :auc, :details)
                """),
                {
                    "timestamp": timestamp,
                    "model_name": "fraud_classifier",
                    "precision": metrics.get("precision"),
                    "recall": metrics.get("recall"),
                    "f1": metrics.get("f1"),
                    "auc": metrics.get("auc"),
                    "details": json.dumps({
                        "data_source": "actual_test_data",
                        "prediction_distribution": {
                            "class_0": 56674,
                            "class_1": 72
                        },
                        "actual_distribution": {
                            "class_0": 56656, 
                            "class_1": 90
                        }
                    })
                },
            )
        print(f"‚úÖ SUCCESS: Logged performance metrics to model_performance table at {timestamp}")
    except Exception as e:
        print(f"‚ùå ERROR logging to model_performance: {e}")

# ‚úÖ Log to data_drift table
def log_to_data_drift(drift_result, engine):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with engine.begin() as conn:
            conn.execute(
                text("""
                    INSERT INTO data_drift 
                    (timestamp, feature_name, drift_score, p_value, details)
                    VALUES (:timestamp, :feature_name, :drift_score, :p_value, :details)
                """),
                {
                    "timestamp": timestamp,
                    "feature_name": "overall_dataset",
                    "drift_score": 0.0 if not drift_result else 1.0,
                    "p_value": 1.0 if not drift_result else 0.0,
                    "details": json.dumps({
                        "drift_detected": drift_result,
                        "comparison": "train_vs_test",
                        "dataset_sizes": {"train": 226980, "test": 56746}
                    })
                },
            )
        print(f"‚úÖ SUCCESS: Logged drift results to data_drift table at {timestamp}")
    except Exception as e:
        print(f"‚ùå ERROR logging to data_drift: {e}")

# ‚úÖ Test MySQL connection
def test_mysql_connection():
    """Test MySQL connection"""
    try:
        engine = create_engine("mysql+pymysql://root:@localhost:3306/fraud_monitoring")
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        print("‚úÖ MySQL connected successfully")
        return engine
    except Exception as e:
        print(f"‚ùå MySQL connection failed: {e}")
        return None

# ‚úÖ Full simulation with actual data
def simulate_data_and_run():
    print("üîÑ Loading model from models/best_model.pkl ...")
    try:
        model = load_model()
        print(f"‚úÖ Model type: {type(model)}")
    except Exception as e:
        print(f"‚ùå ERROR loading model: {e}")
        return

    # Load actual data
    X_train, y_train, X_test, y_test = load_actual_data()
    if X_train is None:
        print("‚ùå Failed to load data, exiting...")
        return

    # ‚úÖ Compute metrics on actual test data
    print("‚öôÔ∏è Calculating model performance on ACTUAL test data...")
    metrics = calculate_model_metrics(model, X_test, y_test)

    # ‚úÖ Run Evidently drift report
    print("üîÑ Running data drift analysis (train vs test)...")
    try:
        report = Report(metrics=[DatasetDriftMetric()])
        report.run(
            reference_data=X_train.assign(target=y_train),
            current_data=X_test.assign(target=y_test)
        )
        
        drift_result = report.as_dict()['metrics'][0]['result']['dataset_drift']
        print(f"üìä Data Drift Detected (train vs test): {drift_result}")
        
    except Exception as e:
        print(f"‚ùå ERROR in drift analysis: {e}")
        drift_result = False

    # ‚úÖ Connect to MySQL
    print("üîÑ Testing MySQL connection...")
    engine = test_mysql_connection()
    
    if engine is not None:
        # ‚úÖ Log to your existing tables
        log_to_model_performance(metrics, engine)
        log_to_data_drift(drift_result, engine)
    else:
        print("üí° Metrics calculated successfully, skipping MySQL logging")

if __name__ == "__main__":
    simulate_data_and_run()